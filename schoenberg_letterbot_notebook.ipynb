{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb1d1ff-6bfc-450b-a78f-baa43f68fb80",
   "metadata": {},
   "source": [
    "# Arnold Sch√∂nberg Correspondence Chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779bc35b-8fb1-4710-a642-603c048e0fdb",
   "metadata": {},
   "source": [
    "Katharina Bleier, GenAI for Humanists 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af01d6",
   "metadata": {},
   "source": [
    "Arnold Sch√∂nberg's correspondence with the music publishers Universal Edition and Verlag Dreililien comprises 1.400 letters. Dating from 1902 to 1951, the letters cover topics such as the production processes of music scores, public relations activities, performances, and general historical issues. Digital editions typically offer not only transcriptions and text-critical analyses of edited sources. They also provide context through commentary and finding aids, such as indexes and full-text searches. The development of a chatbot combines these tools to provide more flexible, user-friendly access to the letters.\n",
    "The project consists of three components: data cleaning and chunking; retrieval-augmented generation (RAG); user interface (streamlit). \n",
    "\n",
    "digital edition: https://www.schoenberg-ue.at\n",
    "chatbot: https://asletterbot-vqmzkltobryzn6ixr8xong.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27ecd7-d3cd-4ddb-a187-fccf8577a5e0",
   "metadata": {},
   "source": [
    "**Importing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f6eab-ce71-4e19-b716-9063ff4e0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3752ba7-1afa-4228-b8c8-5ed958577fda",
   "metadata": {},
   "source": [
    "**Preprocessing input data**\n",
    "\n",
    "The data set consists of a CSV file containing letter IDs and letter texts. It was extracted from XML/TEI files of the letters as part of a previous project, and be will further processed for this one.\n",
    "\n",
    "The cleaning process includes removing whitespaces, fixing fragmented OCR words, applying german-specific OCR corrections for common errors, normalizing line breaks and spacing. \n",
    "\n",
    "Chunking is used to adapt the length of the text to the context window of the LLM. Here, the process of chunking is initiated when the length of the text exceeds 800 characters, which is equivalent to around 180 tokens. Intelligent chunking ensures that the text is always split at the end of a sentence to maintain context. Metadata (e.g. \"chunk_id\") dokuments the relationship between chunks and original letters.\n",
    "Output: CSV file\n",
    "\n",
    "(Code generated with claude.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_correspondence_csv(input_file, output_file='schoenberg_letters_chunks.csv', max_chunk_size=800):\n",
    "    \"\"\"\n",
    "    Process correspondence CSV file for RAG implementation\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input CSV file\n",
    "        output_file: Path for output CSV file\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processed chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and clean data\n",
    "    print(f\"Loading {input_file}...\")\n",
    "    df = pd.read_csv(input_file, header=None, names=['letter_id', 'text'])\n",
    "    \n",
    "    # Remove header row if exists\n",
    "    if df.iloc[0]['letter_id'] == 'Letter ID':\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    # Clean data\n",
    "    df = df.dropna(subset=['letter_id', 'text'])\n",
    "    df = df.drop_duplicates(subset=['letter_id'], keep='first')\n",
    "    print(f\"Loaded {len(df)} letters\")\n",
    "    \n",
    "    # Clean text\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        \n",
    "        # Remove whitespace\n",
    "        text = re.sub(r' {3,}', ' ', text)\n",
    "        \n",
    "        # Fix fragmented words\n",
    "        text = re.sub(r'\\b(\\w) ([a-z]{2,})\\b', r'\\1\\2', text)\n",
    "        \n",
    "        # Common German OCR fixes\n",
    "        ocr_fixes = {\n",
    "            r'\\bge meldet\\b': 'gemeldet',\n",
    "            r'\\bver rechnet\\b': 'verrechnet', \n",
    "            r'\\bDurch f√ºh rung\\b': 'Durchf√ºhrung',\n",
    "            r'\\bEr ledigung\\b': 'Erledigung'\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in ocr_fixes.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # Clean spacing and line breaks\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    df['text_cleaned'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Create chunks\n",
    "    print(\"Creating chunks...\")\n",
    "    chunks = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        letter_id = row['letter_id']\n",
    "        text = row['text_cleaned']\n",
    "        \n",
    "        if pd.isna(text) or len(text.strip()) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(text) <= max_chunk_size:\n",
    "            # Keep short letters as single chunk\n",
    "            chunks.append({\n",
    "                'letter_id': letter_id,\n",
    "                'chunk_id': f\"{letter_id}_001\",\n",
    "                'text': text,\n",
    "                'chunk_type': 'full_letter',\n",
    "                'chunk_index': 1,\n",
    "                'total_chunks': 1\n",
    "            })\n",
    "        else:\n",
    "            # Split long letters by sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "            current_chunk = \"\"\n",
    "            chunk_index = 1\n",
    "            letter_chunks = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "                    letter_chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence\n",
    "                    chunk_index += 1\n",
    "                else:\n",
    "                    current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            if current_chunk.strip():\n",
    "                letter_chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Create chunk records\n",
    "            for i, chunk_text in enumerate(letter_chunks, 1):\n",
    "                chunks.append({\n",
    "                    'letter_id': letter_id,\n",
    "                    'chunk_id': f\"{letter_id}_{i:03d}\",\n",
    "                    'text': chunk_text,\n",
    "                    'chunk_type': 'partial_letter',\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(letter_chunks)\n",
    "                })\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    chunks_df['char_count'] = chunks_df['text'].str.len()\n",
    "    chunks_df['word_count'] = chunks_df['text'].str.split().str.len()\n",
    "    \n",
    "    # Save results\n",
    "    chunks_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"  Total chunks: {len(chunks_df)}\")\n",
    "    print(f\"  Average chunk length: {chunks_df['char_count'].mean():.0f} characters\")\n",
    "    print(f\"  Saved to: {output_file}\")\n",
    "    \n",
    "    return chunks_df\n",
    "\n",
    "# Simple usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Process the file\n",
    "    result = process_correspondence_csv('letters_extract.csv')\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n=== SAMPLE CHUNKS ===\")\n",
    "    for i in range(min(3, len(result))):\n",
    "        chunk = result.iloc[i]\n",
    "        print(f\"\\nChunk {i+1}: {chunk['chunk_id']}\")\n",
    "        print(f\"  Type: {chunk['chunk_type']}\")\n",
    "        print(f\"  Length: {chunk['char_count']} chars\")\n",
    "        print(f\"  Preview: {chunk['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5e34e-9346-408b-bd59-d6b3fb66e37a",
   "metadata": {},
   "source": [
    "**Streamlit Application** \n",
    "\n",
    "The RAG system uses the preprocessed CSV file as a source of information, and users can choose from three OpenAI models for their queries. The chatbot has been designed to provide answers in two different formats, AI-generated continuous text and IDs and quotes from the three most relevant letters. The former facilitates a general understanding of facts by providing a summarised interpretation in natural language. The latter allows verification based on quotes and referencing the specific source.\n",
    "\n",
    "Llamaindex is a framework that supports more intelligent search through the documents by the AI model. The program loads the CSV file, stores the data in a document list, creates a searchable index, returns index and data.\n",
    "\n",
    "The Bot was created with streamlit, so the code handles user input and output, graphical features of the web interface and also contains several error handlings. Three different models allow the user to choose different performance and cost options.\n",
    "\n",
    "(code based on Renato Rocha Souza, Notebooks/Llamaindex (https://github.com/rsouza/GenAI4Humanists/tree/main/Notebooks/LlamaIndex); Streamlit Documantation (https://docs.streamlit.io/get-started))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure page\n",
    "st.set_page_config(page_title=\"Arnold Sch√∂nberg Letter Chatbot\", page_icon=\"üìú\", layout=\"wide\")\n",
    "\n",
    "# Title\n",
    "st.title(\"üìú Arnold Sch√∂nberg Letter Chatbot\")\n",
    "st.write(\"This Chatbot allows you to ask questions about the Correspondence between Arnold Sch√∂nberg and his publishers Universal-Edition and Verlag Dreililien. A digital edition of these letters is availabel at www.schoenberg-ue.at. The chatbot is based on a file that contains letter IDs and letter text, metadata is no included. In addition to natural language interaction, the bot provides IDs and quotes from up to three relevant letters. Most of the letters are written in German.\")\n",
    "st.write(\"Ask questions about the letters\")\n",
    "\n",
    "# Sidebar for configuration\n",
    "with st.sidebar:\n",
    "    st.header(\"Setup\")\n",
    "    openai_api_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
    "    \n",
    "    # Model selection\n",
    "    st.subheader(\"Model Selection\")\n",
    "    model_options = {\n",
    "        \"GPT-4o Mini\": \"gpt-4o-mini\", \n",
    "        \"GPT-4o\": \"gpt-4o\",\n",
    "        \"GPT-3.5 Turbo\": \"gpt-3.5-turbo\"\n",
    "    }\n",
    "    selected_model = st.selectbox(\"Choose OpenAI Model:\", list(model_options.keys()))\n",
    "    \n",
    "    if openai_api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        st.success(\"API Key set!\")\n",
    "    else:\n",
    "        st.warning(\"Please enter your OpenAI API Key\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"index\" not in st.session_state:\n",
    "    st.session_state.index = None\n",
    "\n",
    "if \"csv_data\" not in st.session_state:\n",
    "    st.session_state.csv_data = None\n",
    "\n",
    "# Load and index CSV\n",
    "@st.cache_resource\n",
    "def load_csv_and_create_index(_openai_api_key, _selected_model):\n",
    "    if not _openai_api_key:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Configure LlamaIndex settings\n",
    "        Settings.llm = OpenAI(model=model_options[_selected_model], temperature=0.1)\n",
    "        Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        # Load the CSV file\n",
    "        csv_file = \"schoenberg_letters_chunks.csv\"\n",
    "        if not os.path.exists(csv_file):\n",
    "            st.error(f\"CSV file '{csv_file}' not found. Please make sure it's in the same directory as this script.\")\n",
    "            return None, None\n",
    "            \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Create documents from CSV rows\n",
    "        documents = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text_content = str(row['text'])\n",
    "            letter_id = str(row['letter_id'])\n",
    "            \n",
    "                       \n",
    "            # Create document with metadata\n",
    "            doc = Document(\n",
    "        text=text_content,\n",
    "        metadata={\n",
    "            \"letter_id\": letter_id,\n",
    "            \"chunk_id\": str(row['chunk_id']),\n",
    "            \"chunk_type\": str(row['chunk_type']),\n",
    "            \"chunk_index\": int(row['chunk_index']),\n",
    "            \"total_chunks\": int(row['total_chunks']),\n",
    "            \"char_count\": int(row['char_count']),\n",
    "            \"word_count\": int(row['word_count']),\n",
    "            \"row_index\": idx\n",
    "        }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Create index\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        \n",
    "        return index, df\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading CSV: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Load index if API key is provided\n",
    "if openai_api_key and (st.session_state.index is None or st.button(\"Reload with Selected Model\")):\n",
    "    with st.spinner(f\"Loading and indexing your CSV with {selected_model}... This may take a moment.\"):\n",
    "        st.session_state.index, st.session_state.csv_data = load_csv_and_create_index(openai_api_key, selected_model)\n",
    "        if st.session_state.index:\n",
    "            st.success(f\"CSV loaded successfully with {selected_model}! You can now ask questions.\")\n",
    "            \n",
    "  \n",
    "# Create two columns for the main interface\n",
    "col1, col2 = st.columns([2, 1])\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"Chat\")\n",
    "    \n",
    "    # Display chat messages\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # Chat input\n",
    "    if prompt := st.chat_input(\"Stellen Sie eine Frage zu den Briefen... / Ask a question about the letters...\"):\n",
    "        if not openai_api_key:\n",
    "            st.error(\"Please enter your OpenAI API Key in the sidebar.\")\n",
    "        elif st.session_state.index is None:\n",
    "            st.error(\"Please wait for the CSV to finish loading.\")\n",
    "        else:\n",
    "            # Add user message to chat\n",
    "            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            with st.chat_message(\"user\"):\n",
    "                st.markdown(prompt)\n",
    "\n",
    "            # Generate response\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                with st.spinner(\"Searching through the letters...\"):\n",
    "                    try:\n",
    "                        # Create query engine with more sources for the second window\n",
    "                        query_engine = st.session_state.index.as_query_engine(\n",
    "                            similarity_top_k=5,\n",
    "                            response_mode=\"compact\"\n",
    "                        )\n",
    "                        \n",
    "                        # Get response\n",
    "                        response = query_engine.query(prompt)\n",
    "                        \n",
    "                        # Display response\n",
    "                        st.markdown(str(response))\n",
    "                        \n",
    "                        # Add assistant response to chat\n",
    "                        st.session_state.messages.append({\"role\": \"assistant\", \"content\": str(response)})\n",
    "                        \n",
    "                        # Store source information for the second window\n",
    "                        if hasattr(response, 'source_nodes'):\n",
    "                            st.session_state.last_sources = response.source_nodes[:3]  # Top 3 sources\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Error: {str(e)}\"\n",
    "                        st.error(error_msg)\n",
    "                        st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "\n",
    "if st.button(\"Clear Chat History\"):\n",
    "        st.session_state.messages = []\n",
    "        if hasattr(st.session_state, 'last_sources'):\n",
    "            delattr(st.session_state, 'last_sources')\n",
    "        st.rerun()      \n",
    "        \n",
    "with col2:\n",
    "    st.subheader(\"Source Letters\")\n",
    "    \n",
    "    if hasattr(st.session_state, 'last_sources') and st.session_state.last_sources:\n",
    "        st.write(\"**Top 3 relevant letters:**\")\n",
    "        \n",
    "        for i, source_node in enumerate(st.session_state.last_sources, 1):\n",
    "            with st.expander(f\"Letter {i}: {source_node.metadata.get('letter_id', 'Unknown ID')}\"):\n",
    "                # Show letter ID\n",
    "                st.write(f\"**Letter ID:** {source_node.metadata.get('letter_id', 'Unknown')}\")\n",
    "                \n",
    "                # Show similarity score if available\n",
    "                if hasattr(source_node, 'score'):\n",
    "                    st.write(f\"**Relevance Score:** {source_node.score:.3f}\")\n",
    "                \n",
    "                # Show excerpt from the letter\n",
    "                st.write(\"**Excerpt:**\")\n",
    "                # Limit the text to avoid overwhelming the interface\n",
    "                text_preview = source_node.text[:500] + \"...\" if len(source_node.text) > 500 else source_node.text\n",
    "                st.write(text_preview)\n",
    "    else:\n",
    "        st.write(\"Ask a question to see relevant letter sources here.\")\n",
    "\n",
    "# Instructions in sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"Instructions\")\n",
    "    st.write(\"\"\"\n",
    "    1. Enter your OpenAI API Key above\n",
    "    2. Choose your preferred OpenAI model\n",
    "    3. Make sure 'schoenberg_letters_chunks.csv' is in the same folder as this script\n",
    "    4. Wait for the CSV to load\n",
    "    5. Ask questions in German or English!\n",
    "    \n",
    "    **Model Information:**\n",
    "    - **GPT-4o**: Most capable, best for complex analysis\n",
    "    - **GPT-4o Mini**: Good balance of capability and speed\n",
    "    - **GPT-3.5 Turbo**: Fastest and most economical\n",
    "    \n",
    "    **Example questions (German):**\n",
    "    - Was schreibt Sch√∂nberg √ºber Notation?\n",
    "    - Gibt es Polemik oder Humor in den Briefen?\n",
    "    - Was steht in den Briefen √ºber Pelleas?\n",
    "    - Fasse die wichtigsten Passagen √ºber Vertr√§ge zusammen\n",
    "    \n",
    "    **Example questions (English):**\n",
    "    - Do the letters discuss performances of Pelleas?\n",
    "    - Are there sarcastic passages in the letters?\n",
    "    - Summarize the main legal issues\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    # Show CSV info if loaded\n",
    "    if st.session_state.csv_data is not None:\n",
    "        st.subheader(\"CSV Information\")\n",
    "        st.write(f\"**Rows:** {len(st.session_state.csv_data)}\")\n",
    "        st.write(f\"**Columns:** {list(st.session_state.csv_data.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvai)",
   "language": "python",
   "name": "venvai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
